{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYlaMW2a2VTZ8HULgbvsvd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kassabronald/Competitive-Programming/blob/main/off.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgx57Mr3O_aO",
        "outputId": "2d693c7e-3fee-4b89-bda5-c09ba166e083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/30\n",
            "461/461 [==============================] - 3s 5ms/step - loss: 10742401.0000 - val_loss: 737304.5000\n",
            "Epoch 2/30\n",
            "461/461 [==============================] - 2s 5ms/step - loss: 1871926.5000 - val_loss: 3028389.5000\n",
            "Epoch 3/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 879511.5625 - val_loss: 1705988.8750\n",
            "Epoch 4/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 985701.4375 - val_loss: 882865.6875\n",
            "Epoch 5/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 458050.8125 - val_loss: 226068.7344\n",
            "Epoch 6/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 326895.4062 - val_loss: 64150.4297\n",
            "Epoch 7/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 275336.9688 - val_loss: 382559.3438\n",
            "Epoch 8/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 255266.8125 - val_loss: 43901.1250\n",
            "Epoch 9/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 238985.6562 - val_loss: 184614.2500\n",
            "Epoch 10/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 240345.8438 - val_loss: 111395.1172\n",
            "Epoch 11/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 243094.3438 - val_loss: 92755.9922\n",
            "Epoch 12/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 260273.1562 - val_loss: 316910.5625\n",
            "Epoch 13/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 247188.3594 - val_loss: 74576.8047\n",
            "Epoch 14/30\n",
            "461/461 [==============================] - 2s 5ms/step - loss: 271912.4062 - val_loss: 371567.5938\n",
            "Epoch 15/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 220105.2344 - val_loss: 138502.6719\n",
            "Epoch 16/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 244956.8906 - val_loss: 223042.7969\n",
            "Epoch 17/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 261852.5625 - val_loss: 108811.7344\n",
            "Epoch 18/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 276079.3750 - val_loss: 110515.2266\n",
            "Epoch 19/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 316851.2188 - val_loss: 176541.9062\n",
            "Epoch 20/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 193821.6406 - val_loss: 309862.3125\n",
            "Epoch 21/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 280934.6875 - val_loss: 432771.0000\n",
            "Epoch 22/30\n",
            "461/461 [==============================] - 2s 5ms/step - loss: 215737.6250 - val_loss: 869196.7500\n",
            "Epoch 23/30\n",
            "461/461 [==============================] - 2s 5ms/step - loss: 271280.8438 - val_loss: 41585.6719\n",
            "Epoch 24/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 283283.3438 - val_loss: 266526.0625\n",
            "Epoch 25/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 247451.1875 - val_loss: 460488.5625\n",
            "Epoch 26/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 290815.7812 - val_loss: 981501.6250\n",
            "Epoch 27/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 302978.0938 - val_loss: 669636.8750\n",
            "Epoch 28/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 310603.6562 - val_loss: 397311.6562\n",
            "Epoch 29/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 209183.2656 - val_loss: 654835.3125\n",
            "Epoch 30/30\n",
            "461/461 [==============================] - 2s 4ms/step - loss: 285413.1562 - val_loss: 399917.8125\n",
            "3688/3688 [==============================] - 5s 1ms/step\n",
            "Prediction error: 16.08623443287133\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #if you have a GPU with CUDA installed, this may speed up computation\n",
        "\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATA_ML/train.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#test_data = data[50000:100000]\n",
        "\n",
        "#train_data = data[0:50000]\n",
        "\n",
        "test_data = data[int(2*len(data)/3):]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = data[0:int(2*len(data)/3)]\n",
        "\n",
        "\n",
        "def getXY(df):\n",
        "\n",
        "    \n",
        "    X_num = df.loc[:, ~df.columns.isin(['text', 'urls','hashtags','mentions','timestamp','retweets_count','TweetID'])]\n",
        "    X_text = df.loc[:, ['text']]\n",
        "    y = df.loc[:,['retweets_count']]\n",
        "    \n",
        "    return X_num,X_text,y\n",
        "\n",
        "\n",
        "X_train_n, X_train_t, y_train = getXY(train_data)\n",
        "\n",
        "X_test_n, X_test_t, y_test = getXY(test_data)\n",
        "\n",
        "meansTr =  [X_train_n[col].mean() for col in X_train_n]\n",
        "meansTe  = [X_test_n[col].mean() for col in X_test_n]\n",
        "StdTr = [X_train_n[col].std() for col in X_train_n]\n",
        "StdTe  = [X_test_n[col].std() for col in X_test_n]\n",
        "\n",
        "\n",
        "X_train_n = (X_train_n - meansTr)/StdTr\n",
        "\n",
        "X_test_n = (X_test_n - meansTe)/StdTe\n",
        "\n",
        "\n",
        "\n",
        "def FeedForwardNN(in_dim,out_dim):\n",
        "    model = Sequential()\n",
        "    #model.add(Dense(input_layer_neurons=16, hidden_layer_neurons=32, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(64,input_dim=in_dim,activation='tanh',kernel_initializer='he_normal'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    model.add(Dense(64,activation='tanh'))\n",
        "    # model.add(Dense(out_dim,input_dim=in_dim,activation='linear'))\n",
        "    model.add(Dense(out_dim,activation='linear'))\n",
        "    \n",
        "    return model\n",
        "    \n",
        "model = FeedForwardNN(X_train_n.shape[1],y_train.shape[1])\n",
        "opt = Adam(learning_rate=5*1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "\n",
        "model.fit(x=X_train_n, y=y_train, validation_data=(X_test_n, y_test),epochs=30, batch_size=512)\n",
        "\n",
        "prediction =model.predict(X_test_n)\n",
        "#prediction = [int(value) if value >= 0 else 0 for value in prediction]\n",
        "# print([i for i in prediction if i > 0])\n",
        "# print(y_test.head(10))\n",
        "print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=prediction))\n",
        "\n",
        "\n",
        "\n",
        "# # Dump the results into a file that follows the required Kaggle template\n",
        "# with open(\"Official_pred.txt\", 'w') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"TweetID\", \"retweets_count\"])\n",
        "#     for index, prediction in enumerate(prediction):\n",
        "#         writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(prediction))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# diff = (prediction.flatten()) - (np.array(y_test))\n",
        "# epsilon=0.001\n",
        "# absolutePercentDiff = np.abs((diff/ (0.001+np.array(y_test)))*100)\n",
        "\n",
        "# mean = np.mean(absolutePercentDiff)\n",
        "# std = np.std(absolutePercentDiff)\n",
        "\n",
        "\n",
        "# print(mean, std)\n",
        "    \n",
        "    \n",
        "# modelN = FeedForwardNN(X_train_n.shape[1],5,y_train.shape[1])\n",
        "\n",
        "\n",
        "# loss_function = nn.MSELoss()\n",
        "# optimizer = torch.optim.SGD(modelN.parameters(), lr=0.1) #actually SGD is just GD in this case\n",
        "\n",
        "# losses = []\n",
        "# tr_acc = []\n",
        "# test_acc = []\n",
        "\n",
        "\n",
        "# X_train_n = (torch.from_numpy(X_train_n.to_numpy()).float().to(device))\n",
        "# X_test_n = (torch.from_numpy(X_test_n.to_numpy()).float().to(device))\n",
        "# y_train = (torch.from_numpy(y_train.to_numpy()).float().to(device))\n",
        "# y_test = (torch.from_numpy(y_test.to_numpy()).float().to(device))\n",
        "\n",
        "# for epoch in range(100):\n",
        "#     output=modelN.forward(X_train_n)\n",
        "#     loss = loss_function(output, y_train)\n",
        "#     optimizer.zero_grad() #required since pytorch accumulates the gradients\n",
        "#     loss.backward() #backpropagation step\n",
        "#     optimizer.step() #update the parameters\n",
        "    \n",
        "#     #update loss and accuracy\n",
        "#     losses.append(loss.data)\n",
        "#     output_te=modelN(X_test_n)\n",
        "#     tr_acc.append(abs(y_train.detach().numpy()-output.detach().numpy()).mean())\n",
        "#     test_acc.append(abs(y_test.detach().numpy()-output_te.detach().numpy()).mean())\n",
        "\n",
        "# #     tr_acc.append(accuracy_score(y_train.detach().numpy().astype(np.float32), output.detach().numpy().astype(np.float32)))\n",
        "# #     test_acc.append(accuracy_score(y_test.detach().numpy().astype(np.float32), output_te.detach().numpy().astype(np.float32)))\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,8))\n",
        "# ax1.plot(losses)\n",
        "# ax1.set_title(\"Training Loss\")\n",
        "# ax1.set_xlabel(\"Iterations\")\n",
        "# ax2.plot(test_acc, c='r', label='test')\n",
        "# ax2.plot(tr_acc,  c='b', label='train')\n",
        "# ax2.set_title(\"Train and test accuracy\")\n",
        "# ax2.set_xlabel(\"Iterations\")\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# print(losses[-1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# X_train_t, X_test_t, y_train_t, y_test_t = scsplit(text, text['retweets_count'], stratify=text['retweets_count'], train_size=0.7, test_size=0.3)\n",
        "\n",
        "# X_train_t = X_train_t.drop(['retweets_count'], axis=1)\n",
        "# X_test_t = X_test_t.drop(['retweets_count'], axis=1)\n",
        "# X_train_n = X_train_n.drop(['retweets_count'], axis=1)\n",
        "# X_test_n = X_test_n.drop(['retweets_count'], axis=1)\n",
        "\n",
        "\n",
        "# def createMLP(dimension):\n",
        "    \n",
        "#     model = Sequential()\n",
        "    \n",
        "#     model.add(Dense(8, input_dim=dimension, activation=\"relu\"))\n",
        "#     model.add(Dense(4, activation=\"relu\"))\n",
        "    \n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "# def preProcessText(X_text_train, X_text_test):\n",
        "    \n",
        "#     max_length = 1000\n",
        "#     tokenizer = Tokenizer(num_words = 1000)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     tokenizer.fit_on_texts(X_text_train['text'])\n",
        "    \n",
        "#     embeddings_index = {}\n",
        "    \n",
        "    \n",
        "#     word_index = tokenizer.word_index\n",
        "    \n",
        "#     X_text_train_sequences = tokenizer.texts_to_sequences(X_text_train['text'])\n",
        "    \n",
        "   \n",
        "\n",
        "#     # Use pad_sequences to transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps)\n",
        "#     X_text_train_padded = pad_sequences(X_text_train_sequences,maxlen=max_length)\n",
        "    \n",
        "#     #print(X_text_train_padded)\n",
        "\n",
        "#     # Tokenize and sequence validation data                   \n",
        "#     X_text_test_sequences = tokenizer.texts_to_sequences(X_text_test)\n",
        "\n",
        "#     # pad_sequences for validation set\n",
        "#     X_text_test_padded = pad_sequences(X_text_test_sequences,maxlen=max_length)\n",
        "\n",
        "\n",
        "#     f = open('glove.6B/glove.6B.100d.txt',encoding='utf-8')   \n",
        "#     for line in f:       \n",
        "#        values = line.split()       \n",
        "#        word = values[0]       \n",
        "#        coefs = np.asarray(values[1:], dtype='float32') \n",
        "#        embeddings_index[word] = coefs   \n",
        "#     f.close()\n",
        "\n",
        "#     # Creating embedding matrix for each word in Our corpus\n",
        "#     embedding_matrix = np.zeros((len(word_index) + 1, 100))   \n",
        "#     for word, i in word_index.items():   \n",
        "#        embedding_vector = embeddings_index.get(word)   \n",
        "#        if embedding_vector is not None:   \n",
        "#            # words not found in the embedding index will be all-zeros.  \n",
        "#            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "#     return X_text_train_padded,X_text_test_padded,embedding_matrix,word_index\n",
        "\n",
        "\n",
        "\n",
        "# X_text_train_padded,X_text_test_padded,embedding_matrix,word_index = preProcessText(X_train_t, X_test_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def make_model(max_length,embedding_matrix,word_index):\n",
        "\n",
        "#     # Defining the embedding layer\n",
        "#     max_length = 1000\n",
        "#     embedding_dim = 64 \n",
        "#     input1=Input(shape=(max_length,))\n",
        "    \n",
        "#     embedding_layer = Embedding(len(word_index) + 1,100,weights=[embedding_matrix],input_length=max_length,trainable=False)(input1)\n",
        "                                \n",
        "#     # Building LSTM for text features                          \n",
        "#     bi_lstm_1 = Bidirectional(LSTM(embedding_dim,return_sequences=True))(embedding_layer)\n",
        "    \n",
        "#     bi_lstm_2 = Bidirectional(LSTM(embedding_dim))(bi_lstm_1)   \n",
        "#     lstm_output =  Model(inputs = input1,outputs = bi_lstm_2)\n",
        "    \n",
        "#     #Inputting Number features\n",
        "#     input2=Input(shape=(9,))  \n",
        "    \n",
        "#     # Merging inputs\n",
        "#     merge = concatenate([lstm_output.output,input2])\n",
        "    \n",
        "#     # Building dense layers for regression with number features\n",
        "#     reg_dense1 = Dense(64, activation='relu')(merge)\n",
        "#     reg_dense2 = Dense(16, activation='relu')(reg_dense1) \n",
        "#     output1 = Dense(1, activation='sigmoid')(reg_dense2)\n",
        "    \n",
        "#     # Building dense layers for classification with number features\n",
        "#     clf_dense1 = Dense(64, activation='relu')(merge)\n",
        "#     clf_dense2 = Dense(16, activation='relu')(clf_dense1)\n",
        "    \n",
        "#     # 5 Categories in classification\n",
        "#     output2 = Dense(5, activation='softmax')(clf_dense2)\n",
        " \n",
        "#     model = Model(inputs=[lstm_output.input,input2], outputs=[output1,output2])\n",
        " \n",
        "#     return model\n",
        "\n",
        "\n",
        "# mlp = createMLP(X_train_n.shape[1])\n",
        "# cnn = make_model(1000,embedding_matrix,word_index)\n",
        "\n",
        "# combinedInput = concatenate([mlp.output, cnn.output[0]])\n",
        "# # our final FC layer head will have two dense layers, the final one\n",
        "# # being our regression head\n",
        "# x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "# x = Dense(1, activation=\"linear\")(x)\n",
        "# # our final model will accept categorical/numerical data on the MLP\n",
        "# # input and images on the CNN input, outputting a single value (the\n",
        "# # predicted price of the house)\n",
        "# model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
        "\n",
        "# model.compile(loss=\"mean_absolute_percentage_error\")\n",
        "\n",
        "# model.fit(x=[X_train_n, X_text_train_padded], y=y_train_t,validation_data=([X_test_n, X_text_test_padded], y_test_t),epochs=200, batch_size=8)\n",
        "\n",
        "# preds = model.predict([X_test_n, X_text_test_padded])\n",
        "\n",
        "\n",
        "# diff = preds.flatten() - y_test_t\n",
        "# percentDiff = (diff / y_test_t) * 100\n",
        "# absPercentDiff = np.abs(percentDiff)\n",
        "\n",
        "# mean = np.mean(absPercentDiff)\n",
        "# std = np.std(absPercentDiff)\n",
        "\n",
        "# print(mean, std)\n",
        "\n",
        "# # Now we can train our model. Here we chose a Gradient Boosting Regressor and we set our loss function \n",
        "# reg = GradientBoostingRegressor()#reg = RandomForestRegressor() #\n",
        "# #reg = LinearRegression()\n",
        "\n",
        "# # We fit our model using the training data\n",
        "# reg.fit(X_train, y_train)\n",
        "# # And then we predict the values for our testing set\n",
        "# y_pred = reg.predict(X_test)\n",
        "# # We want to make sure that all predictions are non-negative integers\n",
        "# y_pred = [int(value) if value >= 0 else 0 for value in y_pred]\n",
        "\n",
        "# print(\"Prediction error:\", mean_absolute_error(y_true=y_test, y_pred=y_pred))\n",
        "\n",
        "\n",
        "# ###################################\n",
        "# # Once we finalized our features and model we can train it using the whole training set and then produce prediction for the evaluating dataset\n",
        "# ###################################\n",
        "# # Load the evaluation data\n",
        "# eval_data = pd.read_csv(\"evaluation.csv\")\n",
        "# # Transform our data into tfidf vectors\n",
        "# vectorizer = TfidfVectorizer(max_features=100, stop_words=stopwords.words('french'))\n",
        "# y_train = train_data['retweets_count']\n",
        "# X_train = vectorizer.fit_transform(train_data['text'])\n",
        "# # We fit our model using the training data\n",
        "# reg = GradientBoostingRegressor()\n",
        "# reg.fit(X_train, y_train)\n",
        "# X_val = vectorizer.transform(eval_data['text'])\n",
        "# # Predict the number of retweets for the evaluation dataset\n",
        "# y_pred = reg.predict(X_val)\n",
        "# y_pred = [int(value) if value >= 0 else 0 for value in y_pred]\n",
        "# # Dump the results into a file that follows the required Kaggle template\n",
        "# with open(\"gbr_predictions.txt\", 'w') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"TweetID\", \"retweets_count\"])\n",
        "#     for index, prediction in enumerate(y_pred):\n",
        "#         writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(prediction))])\n",
        "\n",
        "# dummy_regr = DummyRegressor(strategy=\"mean\")\n",
        "# dummy_regr.fit(X_train, y_train)\n",
        "# dummy_pred = dummy_regr.predict(X_val)\n",
        "# with open(\"mean_predictions.txt\", 'w') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"TweetID\", \"retweets_count\"])\n",
        "#     for index, prediction in enumerate(dummy_pred):\n",
        "#         writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(prediction))])\n",
        "\n",
        "# dummy_regr = DummyRegressor(strategy=\"constant\", constant=0)\n",
        "# dummy_regr.fit(X_train, y_train)\n",
        "# dummy_pred = dummy_regr.predict(X_val)\n",
        "\n",
        "# with open(\"zero_predictions.txt\", 'w') as f:\n",
        "#     writer = csv.writer(f)\n",
        "#     writer.writerow([\"TweetID\", \"retweets_count\"])\n",
        "#     for index, prediction in enumerate(dummy_pred):\n",
        "#         writer.writerow([str(eval_data['TweetID'].iloc[index]) , str(int(prediction))])\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}